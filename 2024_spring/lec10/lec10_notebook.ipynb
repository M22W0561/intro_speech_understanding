{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a424b44",
   "metadata": {},
   "source": [
    "# Speech Understanding \n",
    "# Lecture 10: Fourier analysis of natural speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3983e7c",
   "metadata": {},
   "source": [
    "### Mark Hasegawa-Johnson, KCGI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa13a99",
   "metadata": {},
   "source": [
    "* A synthetic vowel can be analyzed by taking the Fourier transform of the whole numpy array\n",
    "* Real speech changes over time\n",
    "* In order to cope with the changes over time, we can use the short-time Fourier transform (STFT)\n",
    "* The log magnitude STFT is called the spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8ca0a6",
   "metadata": {},
   "source": [
    "Here are the contents:\n",
    "1. <a href=\"#section1\">Using ipywebrtc to record speech</a>\n",
    "1. <a href=\"#section2\">Using librosa to read speech audio files</a>\n",
    "1. <a href=\"#section3\">Calculating a spectrogram using `np.fft.fft`</a>\n",
    "1. <a href=\"#section4\">Calculating a spectrogram using librosa</a>\n",
    "1. <a href=\"#homework\">Homework</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5b87c",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc1f1a1",
   "metadata": {},
   "source": [
    "## 1.  Using <a href=\"https://ipywebrtc.readthedocs.io/en/latest/\">ipywebrtc</a> to record speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c958c9dc",
   "metadata": {},
   "source": [
    "[ipywebrtc](https://ipywebrtc.readthedocs.io/en/latest/) is a general package for recording audio and video into your Jupyter notebook.\n",
    "\n",
    "In order to use ipywebrtc, first you need to install it.  You can do that using the following shell command, or by issuing the same command without the \"!\" in a terminal window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e287d45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ipywebrtc\n",
      "  Downloading ipywebrtc-0.6.0-py2.py3-none-any.whl.metadata (825 bytes)\n",
      "Downloading ipywebrtc-0.6.0-py2.py3-none-any.whl (260 kB)\n",
      "   ---------------------------------------- 0.0/260.7 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 122.9/260.7 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 260.7/260.7 kB 3.2 MB/s eta 0:00:00\n",
      "Installing collected packages: ipywebrtc\n",
      "Successfully installed ipywebrtc-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywebrtc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975c563",
   "metadata": {},
   "source": [
    "Recording audio using ipywebrtc is exactly the same as recording video, except that we create a `CameraStream` object with the constraints `'audio':True, 'video':False`.  With those constraints, the camera stream will record only audio, not video.\n",
    "\n",
    "When you the circle button is black, you can press it to start recording.  When it is red, you can press it to stop recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "005827bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywebrtc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mipywebrtc\u001b[39;00m\n\u001b[0;32m      2\u001b[0m camera \u001b[38;5;241m=\u001b[39m ipywebrtc\u001b[38;5;241m.\u001b[39mCameraStream(constraints\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28;01mFalse\u001b[39;00m})\n\u001b[0;32m      3\u001b[0m recorder \u001b[38;5;241m=\u001b[39m ipywebrtc\u001b[38;5;241m.\u001b[39mAudioRecorder(stream\u001b[38;5;241m=\u001b[39mcamera)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ipywebrtc'"
     ]
    }
   ],
   "source": [
    "import ipywebrtc\n",
    "camera = ipywebrtc.CameraStream(constraints={'audio': True,'video':False})\n",
    "recorder = ipywebrtc.AudioRecorder(stream=camera)\n",
    "recorder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0382982",
   "metadata": {},
   "source": [
    "If we print the `recorder` object, we can see its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recorder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddab695",
   "metadata": {},
   "source": [
    "We can see that it is an `AudioRecorder` object, with the following member data:\n",
    "* audio - an audio object, containing\n",
    "   * value - the binary data of the recorded audio waveform\n",
    "   * format='webm' - the audio coding format in which audio.value is stored\n",
    "* stream - the `CameraStream` object from which the audio was recorded\n",
    "\n",
    "There are several ways to convert the `audio.value` data into a numpy array, but one of the simplest and most general is to just save it as a binary `.webm` file, then read it in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5835b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('recording.webm','wb') as f:\n",
    "    f.write(recorder.audio.value)\n",
    "    \n",
    "import os\n",
    "print('recording.webm is a file containing',os.stat('recording.webm').st_size,'bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731a115a",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f8148f",
   "metadata": {},
   "source": [
    "## 2. Using [librosa](https://librosa.org/doc/latest/generated/librosa.feature.melspectrogram.html) to read speech audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500c2dc3",
   "metadata": {},
   "source": [
    "[librosa](https://librosa.org/doc/latest/generated/librosa.feature.melspectrogram.html) is a very general package for synthesizing and analyzing audio files.  For now, let's use it to read in the `.webm` file you just created.  First, you need to install it.  In this line, the `-q` flag prevents pip from printing anything unless there is an error.  If nothing is printed, then you have no errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6fdb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054c56b8",
   "metadata": {},
   "source": [
    "The [librosa.load](https://librosa.org/doc/latest/generated/librosa.load.html#librosa.load) function is able to read a wider variety of audio file formats than any other python package I know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "speech, Fs = librosa.load('recording.webm',sr=16000)\n",
    "speech_time_axis = np.arange(len(speech))/Fs\n",
    "\n",
    "fig = plt.figure(figsize=(14,3),layout='tight')\n",
    "subplot = fig.subplots(1,1)\n",
    "subplot.plot(speech_time_axis,speech)\n",
    "subplot.set_xlabel('Time (seconds)',fontsize=18)\n",
    "subplot.set_title('Recording sampled at %d samples/second'%(Fs),fontsize=18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6953a",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea109e",
   "metadata": {},
   "source": [
    "## 3. Calculating a spectrogram using np.fft.fft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b91f8c",
   "metadata": {},
   "source": [
    "* A Fourier transform finds out which tones are present in a sound\n",
    "* ... but speech changes over time!\n",
    "\n",
    "In order to solve this problem, we use the **short-time Fourier transform**, or STFT.  The STFT has two steps:\n",
    "\n",
    "1. Divide `speech` into overlapping frames, resulting in a matrix\n",
    "1. Take the Fourier transform of each frame to get the **STFT**\n",
    "1. The **spectrogram** is the log magnitude of the STFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93215de8",
   "metadata": {},
   "source": [
    "### 3.1 Divide speech into overlapping frames\n",
    "\n",
    "Typically, we use frames that are about 0.025 seconds long (25 milliseconds), with a step of 0.01 seconds (10 milliseconds).\n",
    "\n",
    "There are an amazing number of different tricky ways to create overlapping frames in python; see [this page](https://stackoverflow.com/questions/2485669/consecutive-overlapping-subsets-of-array-numpy-python) for lots of methods.  The code below is intended to avoid tricky approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7885e134",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Fs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m frame_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.025\u001b[39m\u001b[38;5;241m*\u001b[39mFs)\n\u001b[0;32m      2\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.01\u001b[39m\u001b[38;5;241m*\u001b[39mFs)\n\u001b[0;32m      3\u001b[0m num_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((\u001b[38;5;28mlen\u001b[39m(speech)\u001b[38;5;241m-\u001b[39mframe_length)\u001b[38;5;241m/\u001b[39mstep)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Fs' is not defined"
     ]
    }
   ],
   "source": [
    "frame_length = int(0.025*Fs)\n",
    "step = int(0.01*Fs)\n",
    "num_frames = int((len(speech)-frame_length)/step)\n",
    "print('frame_length is',frame_length,'and step is',step,'and num_frames is',num_frames)\n",
    "\n",
    "speech_frames = np.zeros((frame_length, num_frames))\n",
    "for frame in range(num_frames):\n",
    "    speech_frames[:,frame] = speech[frame*step:frame*step+frame_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb940da8",
   "metadata": {},
   "source": [
    "We can plot this as an image.  Most of the samples will be near zero, and only a few will be strongly negative or strongly positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9cf5dcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m4\u001b[39m),layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m subplot \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m subplot\u001b[38;5;241m.\u001b[39mimshow(speech_frames,aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(14,4),layout='tight')\n",
    "subplot = fig.subplots(1,1)\n",
    "subplot.imshow(speech_frames,aspect='auto',origin='lower')\n",
    "subplot.set_xlabel('Frame number',fontsize=18)\n",
    "subplot.set_ylabel('Sample number',fontsize=18)\n",
    "subplot.set_title('Image of speech frames, 25ms frames with a 10ms step',fontsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1976ef0e",
   "metadata": {},
   "source": [
    "### 3.2 Take the FFT of each frame to get the STFT\n",
    "\n",
    "The STFT is computed by taking the FFT of each frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c5183df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m speech_stft \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfft(speech_frames, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m4\u001b[39m),layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m subplot \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "speech_stft = np.fft.fft(speech_frames, axis=0)\n",
    "\n",
    "fig = plt.figure(figsize=(14,4),layout='tight')\n",
    "subplot = fig.subplots(1,1)\n",
    "subplot.imshow(np.abs(speech_stft[:frame_length//2]),aspect='auto',origin='lower')\n",
    "subplot.set_xlabel('Frame number',fontsize=18)\n",
    "subplot.set_ylabel('Frequency bin',fontsize=18)\n",
    "subplot.set_title('Image of speech frames, 25ms frames with a 10ms step',fontsize=24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea6c34",
   "metadata": {},
   "source": [
    "### 3.3 The spectrogram is the log magnitude STFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd6ad8",
   "metadata": {},
   "source": [
    "As you can see from the plot above, the STFT is close to zero in most frequency bins.  Low-amplitude information can be made more visible by taking the logarithm.\n",
    "\n",
    "#### Standardization: Use decibels, `20*log10(abs(stft))`\n",
    "If you use `np.log` to calculate the logarithm, then your spectrogram units are not obvious.  If you use `20*np.log10`, then you can say that your spectrogram expresses levels in units of decibels.  The <a href=\"https://en.wikipedia.org/wiki/Decibel\">decibel</a> is an international standard way of expressing logarithms.  Since it's an international standard, it's nice to use it.\n",
    "\n",
    "#### Normalization: Normalize maximum to 0dB, Clip minimum to -60dB\n",
    "In order to avoid taking the logarithm of zero, it's a good idea to use `np.amax` and `np.maximum` so that the largest value is 0dB ($=20\\log_{10}(1)$), and the smallest value is -60dB ($=20\\log_{10}(0.001)$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4118e3aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mstft \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(speech_stft)\n\u001b[0;32m      2\u001b[0m speech_spectrogram \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog10(np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0.001\u001b[39m,mstft\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39mamax(mstft)))\n\u001b[0;32m      4\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m4\u001b[39m),layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "mstft = np.abs(speech_stft)\n",
    "speech_spectrogram = 20*np.log10(np.maximum(0.001,mstft/np.amax(mstft)))\n",
    "                            \n",
    "fig = plt.figure(figsize=(14,4),layout='tight')\n",
    "subplot = fig.subplots(1,1)\n",
    "subplot.imshow(speech_spectrogram[:frame_length//2],aspect='auto',origin='lower')\n",
    "subplot.set_xlabel('Frame number',fontsize=18)\n",
    "subplot.set_ylabel('Frequency bin',fontsize=18)\n",
    "subplot.set_title('Image of speech frames, 25ms frames with a 10ms step',fontsize=24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564707ab",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606aa76c",
   "metadata": {},
   "source": [
    "## 4. Calculating a spectrogram using librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecdf67d",
   "metadata": {},
   "source": [
    "librosa contains some useful functions:\n",
    "\n",
    "* <a href=\"https://librosa.org/doc/main/generated/librosa.stft.html\">librosa.stft</a> will chop a signal into frames, and find the STFT\n",
    "* <a href=\"https://librosa.org/doc/main/generated/librosa.amplitude_to_db.html\">librosa.amplitude_to_db</a> converts the magnitude to decibels\n",
    "* <a href=\"https://librosa.org/doc/main/generated/librosa.display.specshow.html#librosa.display.specshow\">librosa.display.specshow</a> has some extra settings that are useful for making useful images of spectrograms; for example, it can label the X-axis in seconds, and the Y-axis in Hertz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "add5997c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[0;32m      3\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.01\u001b[39m\u001b[38;5;241m*\u001b[39mFs)\n\u001b[0;32m      4\u001b[0m framelength \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.025\u001b[39m\u001b[38;5;241m*\u001b[39mFs)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "step = int(0.01*Fs)\n",
    "framelength = int(0.025*Fs)\n",
    "\n",
    "librosa_stft = librosa.stft(speech,hop_length=step,win_length=framelength)\n",
    "librosa_spectrogram = librosa.amplitude_to_db(np.abs(librosa_stft))\n",
    "librosa.display.specshow(librosa_spectrogram,sr=Fs,hop_length=step,x_axis='s',y_axis='hz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e813bc9",
   "metadata": {},
   "source": [
    "<a id=\"homework\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b32d4",
   "metadata": {},
   "source": [
    "## Homework "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9eeaa5",
   "metadata": {},
   "source": [
    "Homework will be graded on Github.com.  Edit the file in this directory called `homework10.py`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5751d6e2",
   "metadata": {},
   "source": [
    "### Homework 10.1: waveform_to_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8c8138f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function waveform_to_frames in module homework10:\n",
      "\n",
      "waveform_to_frames(waveform, frame_length, step)\n",
      "    Chop a waveform into overlapping frames.\n",
      "\n",
      "    @params:\n",
      "    waveform (np.ndarray(N)) - the waveform\n",
      "    frame_length (scalar) - length of the frame, in samples\n",
      "    step (scalar) - step size, in samples\n",
      "\n",
      "    @returns:\n",
      "    frames (np.ndarray((frame_length, num_frames))) - waveform chopped into frames\n",
      "\n",
      "    num_frames should be at least int((len(speech)-frame_length)/step); it may be longer.\n",
      "    For every n and t such that 0 <= t*step+n <= N-1, it should be the case that\n",
      "       frames[n,t] = waveform[t*step+n]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib, homework10\n",
    "importlib.reload(homework10)\n",
    "help(homework10.waveform_to_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "779f2d5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'speech' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(homework10)\n\u001b[1;32m----> 2\u001b[0m frames \u001b[38;5;241m=\u001b[39m homework10\u001b[38;5;241m.\u001b[39mwaveform_to_frames(speech, frame_length, step)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(frames,origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m'\u001b[39m,aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'speech' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(homework10)\n",
    "frames = homework10.waveform_to_frames(speech, frame_length, step)\n",
    "plt.imshow(frames,origin='lower',aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf95bf4",
   "metadata": {},
   "source": [
    "### Homework 10.2: frames_to_stft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06fa71a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function frames_to_stft in module homework10:\n",
      "\n",
      "frames_to_stft(frames)\n",
      "    Take the FFT of every column of the frames matrix.\n",
      "\n",
      "    @params:\n",
      "    frames (np.ndarray((frame_length, num_frames))) - the speech samples (real-valued)\n",
      "\n",
      "    @returns:\n",
      "    stft (np.ndarray((frame_length,num_frames))) - the STFT (complex-valued)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(homework10)\n",
    "help(homework10.frames_to_stft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63efbf9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(homework10)\n\u001b[1;32m----> 2\u001b[0m stft \u001b[38;5;241m=\u001b[39m homework10\u001b[38;5;241m.\u001b[39mframes_to_stft(frames)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39mabs(stft[:frame_length\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m]),origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m'\u001b[39m,aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'frames' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(homework10)\n",
    "stft = homework10.frames_to_stft(frames)\n",
    "plt.imshow(np.abs(stft[:frame_length//2]),origin='lower',aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb726db",
   "metadata": {},
   "source": [
    "### Homework 10.3: stft_to_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77121659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function stft_to_spectrogram in module homework10:\n",
      "\n",
      "stft_to_spectrogram(stft)\n",
      "    Calculate the level, in decibels, of each complex-valued sample of the STFT,\n",
      "    normalized so the highest value is 0dB,\n",
      "    and clipped so that the lowest value is -60dB.\n",
      "\n",
      "    @params:\n",
      "    stft (np.ndarray((frame_length,num_frames))) - STFT (complex-valued)\n",
      "\n",
      "    @returns:\n",
      "    spectrogram (np.ndarray((frame_length,num_frames)) - spectrogram (real-valued)\n",
      "\n",
      "    The spectrogram should be expressed in decibels (20*log10(abs(stft)).\n",
      "    np.amax(spectrogram) should be 0dB.\n",
      "    np.amin(spectrogram) should be no smaller than -60dB.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(homework10)\n",
    "help(homework10.stft_to_spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86b2b544",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stft' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(homework10)\n\u001b[1;32m----> 2\u001b[0m spectrogram \u001b[38;5;241m=\u001b[39m homework10\u001b[38;5;241m.\u001b[39mstft_to_spectrogram(stft)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(spectrogram[:frame_length\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m],origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m'\u001b[39m,aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stft' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(homework10)\n",
    "spectrogram = homework10.stft_to_spectrogram(stft)\n",
    "plt.imshow(spectrogram[:frame_length//2],origin='lower',aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2dbe8c",
   "metadata": {},
   "source": [
    "### Receiving your grade\n",
    "\n",
    "In order to receive a grade for your homework, you need to:\n",
    "\n",
    "1. Run the following code block on your machine.  The result may list some errors, and then in the very last line, it will show a score.  That score (between 0% and 100%) is the grade you have earned so far.  If you want to earn a higher grade, please continue editing `homework3.py`, and then run this code block again.\n",
    "1. When you are happy with your score (e.g., when it reaches 100%), choose `File` $\\Rightarrow$ `Save and Checkpoint`.  Then use `GitHub Desktop` to commit and push your changes.\n",
    "1. Make sure that the 100% shows on your github repo on github.com.  If it doesn't, you will not receive credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "98a99327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.008s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 successes out of 3 tests run\n",
      "Score: 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'grade' from 'C:\\\\Users\\\\é‚¹\\\\Documents\\\\GitHub\\\\intro_speech_understanding\\\\2024_spring\\\\lec10\\\\grade.py'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib, grade\n",
    "importlib.reload(grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9263e4-4b82-4380-8221-d6b1c144a0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
